for i in {1..6}
do
    sbatch -p gpu  --time=48:00:00 --gpus 1 \
    --wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
            cd /home/users/jbdel/scratch/vilmedic
            conda activate vilmedic
            python bin/train.py config/RRS/biorobert_mono.yml \
            dataset.src.tokenizer_max_len=128 \
            dataset.tgt.tokenizer_max_len=80 \
            trainor.batch_size=16 \
            trainor.grad_accu=8 \
            trainor.optimizer=RAdam \
            trainor.optim_params.lr=0.00005 \
            trainor.early_stop_metric="chexbert-5_micro avg_f1-score" \
            validator.batch_size=4 \
            validator.metrics=[ROUGEL,BLEU,METEOR,chexbert] \
            model.cnn.freeze=False \
            name=rrs_mono \
            ckpt_dir=ckpt
            '
done


# PRETRAINED ENC

sbatch -p gpu  --time=48:00:00 --gpus 1 \
--wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
    cd /home/users/jbdel/scratch/vilmedic
    conda activate vilmedic
    python bin/train.py config/RRS/infonce.yml \
                trainor.batch_size=16 \
                trainor.grad_accu=8 \
                trainor.lr_decay_params.factor=0.8 \
                trainor.lr_decay_params.patience=5 \
                trainor.optim_params.lr=1e-4 \
                validator.batch_size=16 \
                ckpt_dir=ckpt \
                name="rrs_infonce"
       '

sbatch -p gpu  --time=48:00:00 --gpus 1 \
--wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
    cd /home/users/jbdel/scratch/vilmedic
    conda activate vilmedic
    python bin/train.py config/RRS/infonce.yml \
                trainor.batch_size=16 \
                trainor.grad_accu=8 \
                trainor.lr_decay_params.factor=0.8 \
                trainor.lr_decay_params.patience=5 \
                trainor.optim_params.lr=1e-4 \
                validator.batch_size=16 \
                model.loss.proto=ConVIRTLoss \
                model.loss.tau=0.1 \
                model.loss.lambda_=0.75 \
                ckpt_dir=ckpt \
                name="rrs_convirt"
       '

# PRETRAINED DEC

sbatch -p gpu  --time=48:00:00 --gpus 1 \
--wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
    cd /home/users/jbdel/scratch/vilmedic
    conda activate vilmedic
    python bin/train.py config/RRS/infonce.yml \
                trainor.batch_size=16 \
                trainor.grad_accu=8 \
                trainor.lr_decay_params.factor=0.8 \
                trainor.lr_decay_params.patience=5 \
                trainor.optim_params.lr=1e-4 \
                validator.batch_size=16 \
                dataset.seq.file=impression.tok \
                ckpt_dir=ckpt \
                name="rrs_infonce_dec"
   '


sbatch -p gpu  --time=48:00:00 --gpus 1 \
--wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
    cd /home/users/jbdel/scratch/vilmedic
    conda activate vilmedic
    python bin/train.py config/RRS/infonce.yml \
                trainor.batch_size=16 \
                trainor.grad_accu=8 \
                trainor.lr_decay_params.factor=0.8 \
                trainor.lr_decay_params.patience=5 \
                trainor.optim_params.lr=1e-4 \
                validator.batch_size=16 \
                model.loss.proto=ConVIRTLoss \
                model.loss.tau=0.1 \
                model.loss.lambda_=0.75 \
                dataset.seq.file=impression.tok \
                ckpt_dir=ckpt \
                name="rrs_convirt_dec"
   '


# USE PRETRAINED ENC

for i in {1..4}
do
    sbatch -p gpu  --time=48:00:00 --gpus 1 \
        --wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
                cd /home/users/jbdel/scratch/vilmedic
                conda activate vilmedic
                python bin/train.py config/RRS/biorobert_mono.yml \
                dataset.src.tokenizer_max_len=128 \
                dataset.tgt.tokenizer_max_len=80 \
                trainor.batch_size=16 \
                trainor.grad_accu=8 \
                trainor.optimizer=RAdam \
                trainor.optim_params.lr=0.00005 \
                trainor.early_stop_metric="chexbert-5_micro avg_f1-score" \
                validator.batch_size=4 \
                validator.metrics=[ROUGEL,BLEU,METEOR,chexbert] \
                model.cnn.freeze=False \
                model.ckpt.enc=ckpt/rrs_infonce/1.502562_9_681680.pth \
                name=rrs_mono_pretrained_enc \
                ckpt_dir=ckpt
                '
done


for i in {1..4}
do
    sbatch -p gpu  --time=48:00:00 --gpus 1 \
        --wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
                cd /home/users/jbdel/scratch/vilmedic
                conda activate vilmedic
                python bin/train.py config/RRS/biorobert_mono.yml \
                dataset.src.tokenizer_max_len=128 \
                dataset.tgt.tokenizer_max_len=80 \
                trainor.batch_size=16 \
                trainor.grad_accu=8 \
                trainor.optimizer=RAdam \
                trainor.optim_params.lr=0.00005 \
                trainor.early_stop_metric="chexbert-5_micro avg_f1-score" \
                validator.batch_size=4 \
                validator.metrics=[ROUGEL,BLEU,METEOR,chexbert] \
                model.cnn.freeze=False \
                model.ckpt.enc=/scratch/users/jbdel/vilmedic/ckpt/rrs_convirt/1.427243_7_922097.pth \
                name=rrs_mono_pretrained_enc_convirt \
                ckpt_dir=ckpt
                '
done

# USE PRETRAINED ENC DEC

for i in {1..4}
do
    sbatch -p gpu  --time=48:00:00 --gpus 1 \
        --wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
                cd /home/users/jbdel/scratch/vilmedic
                conda activate vilmedic
                python bin/train.py config/RRS/biorobert_mono.yml \
                dataset.src.tokenizer_max_len=128 \
                dataset.tgt.tokenizer_max_len=80 \
                trainor.batch_size=16 \
                trainor.grad_accu=8 \
                trainor.optimizer=RAdam \
                trainor.optim_params.lr=0.00005 \
                trainor.early_stop_metric="chexbert-5_micro avg_f1-score" \
                validator.batch_size=4 \
                validator.metrics=[ROUGEL,BLEU,METEOR,chexbert] \
                model.cnn.freeze=False \
                model.ckpt.enc=/scratch/users/jbdel/vilmedic/ckpt/rrs_convirt/1.427243_7_922097.pth \
                model.ckpt.dec=/scratch/users/jbdel/vilmedic/ckpt/rrs_convirt_dec/1.926179_10_254562.pth \
                name=rrs_mono_pretrained_enc_dec_infonce \
                ckpt_dir=ckpt
                '
done


for i in {1..4}
do
    sbatch -p gpu  --time=48:00:00 --gpus 1 \
        --wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
                cd /home/users/jbdel/scratch/vilmedic
                conda activate vilmedic
                python bin/train.py config/RRS/biorobert_mono.yml \
                dataset.src.tokenizer_max_len=128 \
                dataset.tgt.tokenizer_max_len=80 \
                trainor.batch_size=16 \
                trainor.grad_accu=8 \
                trainor.optimizer=RAdam \
                trainor.optim_params.lr=0.00005 \
                trainor.early_stop_metric="chexbert-5_micro avg_f1-score" \
                validator.batch_size=4 \
                validator.metrics=[ROUGEL,BLEU,METEOR,chexbert] \
                model.cnn.freeze=False \
                model.ckpt.enc=ckpt/rrs_infonce/1.502562_9_681680.pth \
                model.ckpt.dec=ckpt/rrs_infonce_dec/1.954212_8_493043.pth \
                name=rrs_mono_pretrained_enc_dec_convirt \
                ckpt_dir=ckpt
                '
done



python bin/ensemble.py config/RRS/biorobert_mono.yml \
                dataset.src.tokenizer_max_len=128 \
                dataset.tgt.tokenizer_max_len=80 \
                ensemblor.batch_size=4 \
                ensemblor.metrics=[ROUGEL,BLEU,METEOR,chexbert] \
                ensemblor.splits=[test_stanford,test_indiana] \
                ensemblor.mode=best-1 \
                model.cnn.freeze=False \
                name=rrs_mono_pretrained_enc_convirt \
                dataset.src.root=data/RRS/mediaqa/ \
                dataset.tgt.root=data/RRS/mediaqa/ \
                ckpt_dir=ckpt


sbatch -p gpu  --time=5:00:00 --gpus 1 -C GPU_MEM:16GB \
--wrap 'source /home/users/jbdel/scratch/miniconda3/etc/profile.d/conda.sh
    cd /home/users/jbdel/scratch/vilmedic
    conda activate vilmedic
    python bin/train.py config/RRS/biorobert_mono.yml \
            dataset.src.tokenizer_max_len=128 \
            dataset.tgt.tokenizer_max_len=80 \
            trainor.batch_size=8 \
            trainor.optimizer=RAdam \
            trainor.optim_params.lr=5e-6 \
            trainor.early_stop_metric=ROUGEL \
            validator.batch_size=4 \
            validator.metrics=[ROUGEL] \
            model.proto=SumHugMono_SCST \
            model.ckpt=ckpt/indiana_rrs_mono/64.89_5_658183.pth \
            model.score=rouge \
            model.top_k=0 \
            dataset.src.root=data/RRS/mediaqa/indiana \
            dataset.tgt.root=data/RRS/mediaqa/indiana \
            name=indiana_rrs_mono \
            ckpt_dir=ckpt
            '


python bin/ensemble.py config/RRS/biorobert_mono.yml \
                dataset.src.tokenizer_max_len=128 \
                dataset.tgt.tokenizer_max_len=80 \
                ensemblor.batch_size=4 \
                ensemblor.metrics=[ROUGEL,BLEU,METEOR,chexbert] \
                ensemblor.splits=[validate] \
                ensemblor.mode=best-1 \
                name=indiana_rrs_mono \
                    model.proto=SumHugMono_SCST \
                    model.score=rouge \
                dataset.src.root=data/RRS/mediaqa/indiana \
                dataset.tgt.root=data/RRS/mediaqa/indiana \
                ckpt_dir=ckpt

