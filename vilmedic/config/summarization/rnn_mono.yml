name: monomodal_mimic
ckpt_dir: ckpt

dataset:
  proto: Seq2Seq
  src:
    root: data/report_sum/mimic-cxr/
    file: findings.tok
    max_len: 80
  tgt:
    root: data/report_sum/mimic-cxr/
    file: impression.tok
    max_len: 80


model:
  proto: SumRNN

  encoder:
    proto: TextEncoder
    rnn_type: GRU
    input_size: 200
    hidden_size: 320
    n_vocab: 8832
    num_layers: 2
    bidirectional: True
    proj_dim: False
    proj_activ: None
    layer_norm: False
    dropout_rnn: 0
    dropout_emb: 0.4
    dropout_ctx: 0.5

  decoder:
    proto: ConditionalDecoder
    input_size: 200
    hidden_size: 320
    n_vocab: 6867 # impression
    rnn_type: GRU
    tied_emb: True
    dec_init: zero
    dec_init_activ: tanh
    dec_init_size: None
    att_type: mlp
    att_activ: tanh
    att_bottleneck: hid
    att_temp: 1.0
    transform_ctx: True
    mlp_bias: False
    dropout_out: 0.5


trainor:
  optimizer: Adam
  optim_params: {lr: 0.0004, weight_decay: 0.00001}
  batch_size: 64
  lr_decay: ReduceLROnPlateau
  lr_decay_params:
    factor: 0.5
    patience: 1
    min_lr: 0.000001
    threshold_mode: abs
  epochs: 99
  early_stop: 10
  eval_start: 0
  early_stop_metric: ROUGE2

validator:
  batch_size: 16
  beam_width: 8
  metrics: [ROUGE2, BLEU, METEOR, ROUGEL]
  splits: [validate]


ensemblor:
  batch_size: 16
  beam_width: 8
  metrics: [ROUGE2, BLEU, METEOR, ROUGEL]
  splits: [validate, test]
  mode: all
