name: dalle
ckpt_dir: ckpt

dataset:
  proto: ImSeq
  image:
    root: data/CLIP/mimic-cxr/
    file: image.tok
    image_path: data/images/
    load_memory: False
    ext: .jpg
    custom_transform_train: >
      transforms.Compose([
                      transforms.Resize(256),
                      transforms.CenterCrop(256),
                      transforms.ToTensor()])
    custom_transform_validate: >
      transforms.Compose([
                      transforms.Resize(256),
                      transforms.CenterCrop(256),
                      transforms.ToTensor()])

  seq:
    root: data/CLIP/mimic-cxr/
    file: impression.tok
    tokenizer_max_len: 100
    processing: r2gen_clean_report

model:
  proto: DALLE
  vae:
    image_size: 256
    num_layers: 3
    num_tokens: 8192
    codebook_dim: 1024
    hidden_dim: 128
    num_resnet_blocks: 1
    temperature: 0.9
    straight_through: False

  dalle:
    dim: 1024
    num_text_tokens: 6867     # vocab size for text
    text_seq_len: 100         # text sequence length
    depth: 12                 # should aim to be 64
    heads: 16                 # attention heads
    dim_head: 64              # attention head dimension
    attn_dropout: 0.1         # attention dropout
    ff_dropout: 0.1

trainor:
  optimizer: RAdam
  optim_params: {lr: 3e-4}
  batch_size: 2
  lr_decay: ReduceLROnPlateau
  lr_decay_params:
    factor: 0.8
    patience: 0
    min_lr: 0.000001
    threshold: 0.01
    threshold_mode: abs
  epochs: 99
  early_stop: 10
  eval_start: 0
  early_stop_metric: validation_loss

validator:
  batch_size: 8
  metrics: []
  splits: [validate]


ensemblor:
  generate_images: True
  num_images: 5
  batch_size: 2
  metrics: []
  splits: [validate]
  mode: all # best,all