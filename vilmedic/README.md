
<p align="center">
  <img src="docs/logo.png" width="150">
  <br />
  <br />
  <a href="https://vilmedic.readthedocs.io/en/latest/">
  <img alt="Documentation Status" src="https://readthedocs.org/projects/vilmedic/badge/?version=latest"/>
  </a>
   <a href="https://github.com/pytorch/fairseq/blob/master/LICENSE"><img alt="MIT License" src="https://img.shields.io/badge/license-MIT-red.svg" /></a>
  <img src="https://img.shields.io/badge/Stanford-Medicine-red" />
</p>

---

ViLMedic (Vision-and-Language medical research) is a modular framework for vision and language multimodal research in the medical field. 
ViLMedic contains reference implementations of state-of-the-art vision and language architectures, referred as "blocks" and full solutions for multimodal medical tasks using one or several blocks.
See full list of solutions offered by [ViLMedic]().


## Installation
```
git clone  https://github.com/jbdel/vilmedic
pip install -r requirements.txt
```


## Documentation

Learn more about ViLMedic [here](https://vilmedic.readthedocs.io/en/latest/).

## Citation

If you use ViLMedic in your work or use any models published in ViLMedic, please cite:

```bibtex
@misc{Delbrouck2021ViLmedic,
  author =       {Delbrouck, Jean-Benoit},
  title =        {ViLMedic: A multimodal framework for vision and language medical research},
  howpublished = {\url{https://github.com/jbdel/vilmedic}},
  year =         {2021}
}
```

## License
ViLMedic is MIT-licensed. The license applies to the pre-trained models as well.